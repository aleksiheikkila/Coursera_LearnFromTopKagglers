{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.13.1\n",
      "pandas 0.20.3\n",
      "scipy 0.19.1\n",
      "sklearn 0.19.0\n",
      "lightgbm 2.0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use only 3 shops for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "    #Cartesian product of input iterables.  Equivalent to nested for-loops.\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>7738</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>7737</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>7770</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7664</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7814</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7057.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item\n",
       "0       28     7738               0     4.0       7057.0         11.0\n",
       "1       28     7737               0    10.0       7057.0         16.0\n",
       "2       28     7770               0     6.0       7057.0         10.0\n",
       "3       28     7664               0     1.0       7057.0          1.0\n",
       "4       28     7814               0     2.0       7057.0          6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()  # every shop, item, Month... Mo sum of sales, + Mo sum of sales for the shop + month sum sales item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32f295056ec4eef83bd030b44752975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm_notebook(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)   \n",
    "    # nyt train_shiftissä tuo kk on lägätty. Joinaituu oikeaan kohtaan\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "# add it to the df\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>target</th>\n",
       "      <th>target_shop</th>\n",
       "      <th>target_item</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>10994</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>10992</td>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>10991</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7521.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>10988</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5609.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11002</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6949.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shop_id  item_id  date_block_num  target  target_shop  target_item  \\\n",
       "0       28    10994              12     1.0       6949.0          1.0   \n",
       "1       28    10992              12     3.0       6949.0          4.0   \n",
       "2       28    10991              12     1.0       6949.0          5.0   \n",
       "3       28    10988              12     1.0       6949.0          2.0   \n",
       "4       28    11002              12     1.0       6949.0          1.0   \n",
       "\n",
       "   target_lag_1  target_item_lag_1  target_shop_lag_1  target_lag_2  \\\n",
       "0           0.0                1.0             8499.0           0.0   \n",
       "1           3.0                7.0             8499.0           0.0   \n",
       "2           1.0                3.0             8499.0           0.0   \n",
       "3           2.0                5.0             8499.0           4.0   \n",
       "4           0.0                1.0             8499.0           0.0   \n",
       "\n",
       "   target_item_lag_2  target_shop_lag_2  target_lag_3  target_item_lag_3  \\\n",
       "0                1.0             6454.0           0.0                0.0   \n",
       "1                0.0                0.0           0.0                0.0   \n",
       "2                0.0                0.0           0.0                1.0   \n",
       "3                5.0             6454.0           5.0                6.0   \n",
       "4                0.0                0.0           0.0                0.0   \n",
       "\n",
       "   target_shop_lag_3  target_lag_4  target_item_lag_4  target_shop_lag_4  \\\n",
       "0                0.0           0.0                0.0                0.0   \n",
       "1                0.0           0.0                0.0                0.0   \n",
       "2             5609.0           0.0                2.0             6753.0   \n",
       "3             5609.0           0.0                2.0             6753.0   \n",
       "4                0.0           0.0                0.0                0.0   \n",
       "\n",
       "   target_lag_5  target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "0           0.0                0.0                0.0            0.0   \n",
       "1           0.0                1.0             7521.0            0.0   \n",
       "2           2.0                4.0             7521.0            0.0   \n",
       "3           0.0                0.0                0.0            0.0   \n",
       "4           0.0                0.0                0.0            0.0   \n",
       "\n",
       "   target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "0                 0.0                 0.0                37  \n",
       "1                 0.0                 0.0                37  \n",
       "2                 0.0                 0.0                40  \n",
       "3                 0.0                 0.0                40  \n",
       "4                 0.0                 0.0                40  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]  # test is the last month\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        12\n",
       "1        12\n",
       "2        12\n",
       "3        12\n",
       "4        12\n",
       "5        12\n",
       "6        12\n",
       "7        12\n",
       "8        12\n",
       "9        12\n",
       "10       12\n",
       "11       12\n",
       "12       12\n",
       "13       12\n",
       "14       12\n",
       "15       12\n",
       "16       12\n",
       "17       12\n",
       "18       12\n",
       "19       12\n",
       "20       12\n",
       "21       12\n",
       "22       12\n",
       "23       12\n",
       "24       12\n",
       "25       12\n",
       "26       12\n",
       "27       12\n",
       "28       12\n",
       "29       12\n",
       "30       12\n",
       "31       12\n",
       "32       12\n",
       "33       12\n",
       "34       12\n",
       "35       12\n",
       "36       12\n",
       "37       12\n",
       "38       12\n",
       "39       12\n",
       "40       12\n",
       "41       12\n",
       "42       12\n",
       "43       12\n",
       "44       12\n",
       "45       12\n",
       "46       12\n",
       "47       12\n",
       "48       12\n",
       "49       12\n",
       "50       12\n",
       "51       12\n",
       "52       12\n",
       "53       12\n",
       "54       12\n",
       "55       12\n",
       "56       12\n",
       "57       12\n",
       "58       12\n",
       "59       12\n",
       "60       12\n",
       "61       12\n",
       "62       12\n",
       "63       12\n",
       "64       12\n",
       "65       12\n",
       "66       12\n",
       "67       12\n",
       "68       12\n",
       "69       12\n",
       "70       12\n",
       "71       12\n",
       "72       12\n",
       "73       12\n",
       "74       12\n",
       "75       12\n",
       "76       12\n",
       "77       12\n",
       "78       12\n",
       "79       12\n",
       "80       12\n",
       "81       12\n",
       "82       12\n",
       "83       12\n",
       "84       12\n",
       "85       12\n",
       "86       12\n",
       "87       12\n",
       "88       12\n",
       "89       12\n",
       "90       12\n",
       "91       12\n",
       "92       12\n",
       "93       12\n",
       "94       12\n",
       "95       12\n",
       "96       12\n",
       "97       12\n",
       "98       12\n",
       "99       12\n",
       "100      12\n",
       "101      12\n",
       "102      12\n",
       "103      12\n",
       "104      12\n",
       "105      12\n",
       "106      12\n",
       "107      12\n",
       "108      12\n",
       "109      12\n",
       "110      12\n",
       "111      12\n",
       "112      12\n",
       "113      12\n",
       "114      12\n",
       "115      12\n",
       "116      12\n",
       "117      12\n",
       "118      12\n",
       "119      12\n",
       "120      12\n",
       "121      12\n",
       "122      12\n",
       "123      12\n",
       "124      12\n",
       "125      12\n",
       "126      12\n",
       "127      12\n",
       "128      12\n",
       "129      12\n",
       "130      12\n",
       "131      12\n",
       "132      12\n",
       "133      12\n",
       "134      12\n",
       "135      12\n",
       "136      12\n",
       "137      12\n",
       "138      12\n",
       "139      12\n",
       "140      12\n",
       "141      12\n",
       "142      12\n",
       "143      12\n",
       "144      12\n",
       "145      12\n",
       "146      12\n",
       "147      12\n",
       "148      12\n",
       "149      12\n",
       "150      12\n",
       "151      12\n",
       "152      12\n",
       "153      12\n",
       "154      12\n",
       "155      12\n",
       "156      12\n",
       "157      12\n",
       "158      12\n",
       "159      12\n",
       "160      12\n",
       "161      12\n",
       "162      12\n",
       "163      12\n",
       "164      12\n",
       "165      12\n",
       "166      12\n",
       "167      12\n",
       "168      12\n",
       "169      12\n",
       "170      12\n",
       "171      12\n",
       "172      12\n",
       "173      12\n",
       "174      12\n",
       "175      12\n",
       "176      12\n",
       "177      12\n",
       "178      12\n",
       "179      12\n",
       "180      12\n",
       "181      12\n",
       "182      12\n",
       "183      12\n",
       "184      12\n",
       "185      12\n",
       "186      12\n",
       "187      12\n",
       "188      12\n",
       "189      12\n",
       "190      12\n",
       "191      12\n",
       "192      12\n",
       "193      12\n",
       "194      12\n",
       "195      12\n",
       "196      12\n",
       "197      12\n",
       "198      12\n",
       "199      12\n",
       "200      12\n",
       "201      12\n",
       "202      12\n",
       "203      12\n",
       "204      12\n",
       "205      12\n",
       "206      12\n",
       "207      12\n",
       "208      12\n",
       "209      12\n",
       "210      12\n",
       "211      12\n",
       "212      12\n",
       "213      12\n",
       "214      12\n",
       "215      12\n",
       "216      12\n",
       "217      12\n",
       "218      12\n",
       "219      12\n",
       "220      12\n",
       "221      12\n",
       "222      12\n",
       "223      12\n",
       "224      12\n",
       "225      12\n",
       "226      12\n",
       "227      12\n",
       "228      12\n",
       "229      12\n",
       "230      12\n",
       "231      12\n",
       "232      12\n",
       "233      12\n",
       "234      12\n",
       "235      12\n",
       "236      12\n",
       "237      12\n",
       "238      12\n",
       "239      12\n",
       "240      12\n",
       "241      12\n",
       "242      12\n",
       "243      12\n",
       "244      12\n",
       "245      12\n",
       "246      12\n",
       "247      12\n",
       "248      12\n",
       "249      12\n",
       "250      12\n",
       "251      12\n",
       "252      12\n",
       "253      12\n",
       "254      12\n",
       "255      12\n",
       "256      12\n",
       "257      12\n",
       "258      12\n",
       "259      12\n",
       "260      12\n",
       "261      12\n",
       "262      12\n",
       "263      12\n",
       "264      12\n",
       "265      12\n",
       "266      12\n",
       "267      12\n",
       "268      12\n",
       "269      12\n",
       "270      12\n",
       "271      12\n",
       "272      12\n",
       "273      12\n",
       "274      12\n",
       "275      12\n",
       "276      12\n",
       "277      12\n",
       "278      12\n",
       "279      12\n",
       "280      12\n",
       "281      12\n",
       "282      12\n",
       "283      12\n",
       "284      12\n",
       "285      12\n",
       "286      12\n",
       "287      12\n",
       "288      12\n",
       "289      12\n",
       "290      12\n",
       "291      12\n",
       "292      12\n",
       "293      12\n",
       "294      12\n",
       "295      12\n",
       "296      12\n",
       "297      12\n",
       "298      12\n",
       "299      12\n",
       "         ..\n",
       "26322    14\n",
       "26323    14\n",
       "26324    14\n",
       "26325    14\n",
       "26326    14\n",
       "26327    14\n",
       "26328    14\n",
       "26329    14\n",
       "26330    14\n",
       "26331    14\n",
       "26332    14\n",
       "26333    14\n",
       "26334    14\n",
       "26335    14\n",
       "26336    14\n",
       "26337    14\n",
       "26338    14\n",
       "26339    14\n",
       "26340    14\n",
       "26341    14\n",
       "26342    14\n",
       "26343    14\n",
       "26344    14\n",
       "26345    14\n",
       "26346    14\n",
       "26347    14\n",
       "26348    14\n",
       "26349    14\n",
       "26350    14\n",
       "26351    14\n",
       "26352    14\n",
       "26353    14\n",
       "26354    14\n",
       "26355    14\n",
       "26356    14\n",
       "26357    14\n",
       "26358    14\n",
       "26359    14\n",
       "26360    14\n",
       "26361    14\n",
       "26362    14\n",
       "26363    14\n",
       "26364    14\n",
       "26365    14\n",
       "26366    14\n",
       "26367    14\n",
       "26368    14\n",
       "26369    14\n",
       "26370    14\n",
       "26371    14\n",
       "26372    14\n",
       "26373    14\n",
       "26374    14\n",
       "26375    14\n",
       "26376    14\n",
       "26377    14\n",
       "26378    14\n",
       "26379    14\n",
       "26380    14\n",
       "26381    14\n",
       "26382    14\n",
       "26383    14\n",
       "26384    14\n",
       "26385    14\n",
       "26386    14\n",
       "26387    14\n",
       "26388    14\n",
       "26389    14\n",
       "26390    14\n",
       "26391    14\n",
       "26392    14\n",
       "26393    14\n",
       "26394    14\n",
       "26395    14\n",
       "26396    14\n",
       "26397    14\n",
       "26398    14\n",
       "26399    14\n",
       "26400    14\n",
       "26401    14\n",
       "26402    14\n",
       "26403    14\n",
       "26404    14\n",
       "26405    14\n",
       "26406    14\n",
       "26407    14\n",
       "26408    14\n",
       "26409    14\n",
       "26410    14\n",
       "26411    14\n",
       "26412    14\n",
       "26413    14\n",
       "26414    14\n",
       "26415    14\n",
       "26416    14\n",
       "26417    14\n",
       "26418    14\n",
       "26419    14\n",
       "26420    14\n",
       "26421    14\n",
       "26422    14\n",
       "26423    14\n",
       "26424    14\n",
       "26425    14\n",
       "26426    14\n",
       "26427    14\n",
       "26428    14\n",
       "26429    14\n",
       "26430    14\n",
       "26431    14\n",
       "26432    14\n",
       "26433    14\n",
       "26434    14\n",
       "26435    14\n",
       "26436    14\n",
       "26437    14\n",
       "26438    14\n",
       "26439    14\n",
       "26440    14\n",
       "26441    14\n",
       "26442    14\n",
       "26443    14\n",
       "26444    14\n",
       "26445    14\n",
       "26446    14\n",
       "26447    14\n",
       "26448    14\n",
       "26449    14\n",
       "26450    14\n",
       "26451    14\n",
       "26452    14\n",
       "26453    14\n",
       "26454    14\n",
       "26455    14\n",
       "26456    14\n",
       "26457    14\n",
       "26458    14\n",
       "26459    14\n",
       "26460    14\n",
       "26461    14\n",
       "26462    14\n",
       "26463    14\n",
       "26464    14\n",
       "26465    14\n",
       "26466    14\n",
       "26467    14\n",
       "26468    14\n",
       "26469    14\n",
       "26470    14\n",
       "26471    14\n",
       "26472    14\n",
       "26473    14\n",
       "26474    14\n",
       "26475    14\n",
       "26476    14\n",
       "26477    14\n",
       "26478    14\n",
       "26479    14\n",
       "26480    14\n",
       "26481    14\n",
       "26482    14\n",
       "26483    14\n",
       "26484    14\n",
       "26485    14\n",
       "26486    14\n",
       "26487    14\n",
       "26488    14\n",
       "26489    14\n",
       "26490    14\n",
       "26491    14\n",
       "26492    14\n",
       "26493    14\n",
       "26494    14\n",
       "26495    14\n",
       "26496    14\n",
       "26497    14\n",
       "26498    14\n",
       "26499    14\n",
       "26500    14\n",
       "26501    14\n",
       "26502    14\n",
       "26503    14\n",
       "26504    14\n",
       "26505    14\n",
       "26506    14\n",
       "26507    14\n",
       "26508    14\n",
       "26509    14\n",
       "26510    14\n",
       "26511    14\n",
       "26512    14\n",
       "26513    14\n",
       "26514    14\n",
       "26515    14\n",
       "26516    14\n",
       "26517    14\n",
       "26518    14\n",
       "26519    14\n",
       "26520    14\n",
       "26521    14\n",
       "26522    14\n",
       "26523    14\n",
       "26524    14\n",
       "26525    14\n",
       "26526    14\n",
       "26527    14\n",
       "26528    14\n",
       "26529    14\n",
       "26530    14\n",
       "26531    14\n",
       "26532    14\n",
       "26533    14\n",
       "26534    14\n",
       "26535    14\n",
       "26536    14\n",
       "26537    14\n",
       "26538    14\n",
       "26539    14\n",
       "26540    14\n",
       "26541    14\n",
       "26542    14\n",
       "26543    14\n",
       "26544    14\n",
       "26545    14\n",
       "26546    14\n",
       "26547    14\n",
       "26548    14\n",
       "26549    14\n",
       "26550    14\n",
       "26551    14\n",
       "26552    14\n",
       "26553    14\n",
       "26554    14\n",
       "26555    14\n",
       "26556    14\n",
       "26557    14\n",
       "26558    14\n",
       "26559    14\n",
       "26560    14\n",
       "26561    14\n",
       "26562    14\n",
       "26563    14\n",
       "26564    14\n",
       "26565    14\n",
       "26566    14\n",
       "26567    14\n",
       "26568    14\n",
       "26569    14\n",
       "26570    14\n",
       "26571    14\n",
       "26572    14\n",
       "26573    14\n",
       "26574    14\n",
       "26575    14\n",
       "26576    14\n",
       "26577    14\n",
       "26578    14\n",
       "26579    14\n",
       "26580    14\n",
       "26581    14\n",
       "26582    14\n",
       "26583    14\n",
       "26584    14\n",
       "26585    14\n",
       "26586    14\n",
       "26587    14\n",
       "26588    14\n",
       "26589    14\n",
       "26590    14\n",
       "26591    14\n",
       "26592    14\n",
       "26593    14\n",
       "26594    14\n",
       "26595    14\n",
       "26596    14\n",
       "26597    14\n",
       "26598    14\n",
       "26599    14\n",
       "26600    14\n",
       "26601    14\n",
       "26602    14\n",
       "26603    14\n",
       "26604    14\n",
       "26605    14\n",
       "26606    14\n",
       "26607    14\n",
       "26608    14\n",
       "26609    14\n",
       "26610    14\n",
       "26611    14\n",
       "26612    14\n",
       "26613    14\n",
       "26614    14\n",
       "26615    14\n",
       "26616    14\n",
       "26617    14\n",
       "26618    14\n",
       "26619    14\n",
       "26620    14\n",
       "26621    14\n",
       "Name: date_block_num, Length: 26622, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_train[dates_train < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()\n",
    "#y_train[[3,9, 123]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743180\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "# fit on train, predict test\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.738391\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb]   # concatenation along the second axis. \"Add columns...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models.\n",
    "\n",
    "Scheme f:\n",
    "f) KFold scheme in time series:\n",
    "\n",
    "In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
    "\n",
    "    Split the train data into chunks of duration T. Select first M chunks.\n",
    "    Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "    Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "       29, 30, 31, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates_train.unique())\n",
    "dates_train.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34404,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dates_train_level2\n",
    "y_train_level2.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "[ 1.50148988  1.38811989]\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]:\n",
    "    \n",
    "    print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    # M first chunks\n",
    "    traindates_chunk = dates_train < cur_block_num  # aiemmat\n",
    "    preddates_chunk = dates_train == cur_block_num\n",
    "    \n",
    "    lr.fit(X_train[traindates_chunk].values, y_train[traindates_chunk])\n",
    "    pred_lr = lr.predict(X_train[preddates_chunk].values)\n",
    "    \n",
    "    model = lgb.train(lgb_params, lgb.Dataset(X_train[traindates_chunk], label=y_train[traindates_chunk]), 100)\n",
    "    pred_lgb = model.predict(X_train[preddates_chunk])\n",
    "    # Okay, there they are for one chunk.\n",
    "\n",
    "    X_train_level2 = np.vstack([X_train_level2, np.c_[pred_lr, pred_lgb]])\n",
    "    \n",
    "\n",
    "    #  YOUR CODE GOES HERE\n",
    "X_train_level2 = X_train_level2[y_train_level2.shape[0]:]    \n",
    "    \n",
    "# Sanity check\n",
    "print(X_train_level2.mean(axis=0))\n",
    "\n",
    "assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70192199,  0.74784972,  0.89973551,  0.86362637,  1.53796047])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_level2[5:10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.84910119],\n",
       "       [ 0.84910119,  1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFpCAYAAABTSWtMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa306a69e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1], alpha=0.5, s=10)\n",
    "\n",
    "np.corrcoef(X_train_level2[:,0], X_train_level2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.765000; Corresponding r2 score on train: 0.627255\n"
     ]
    }
   ],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "# so try alphas, compute mix in X_train_level2, calculate r2, track the best alpha\n",
    "r2_train_simple_mix = -999\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha * X_train_level2[:,0] + (1-alpha) * X_train_level2[:,1]\n",
    "    # calc R2 score\n",
    "    r2 = r2_score(y_train_level2, mix)\n",
    "    if r2 > r2_train_simple_mix:\n",
    "        r2_train_simple_mix = r2\n",
    "        best_alpha = alpha\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for simple mix is 0.781144\n"
     ]
    }
   ],
   "source": [
    "test_preds = best_alpha * X_test_level2[:,0] + (1-best_alpha) * X_test_level2[:,1]# YOUR CODE GOES HERE\n",
    "r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "\n",
    "lr_stacker = LinearRegression()\n",
    "lr_stacker.fit(X_train_level2, y_train_level2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.632176\n",
      "Test  R-squared for stacking is 0.771297\n"
     ]
    }
   ],
   "source": [
    "train_preds = lr_stacker.predict(X_train_level2)# YOUR CODE GOES HERE\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "test_preds = lr_stacker.predict(X_test_level2)# YOUR CODE GOES HERE\n",
    "r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task best_alpha is: 0.765\n",
      "Current answer for task r2_train_simple_mix is: 0.627255043446\n",
      "Current answer for task r2_test_simple_mix is: 0.781144169579\n",
      "Current answer for task r2_train_stacking is: 0.632175561459\n",
      "Current answer for task r2_test_stacking is: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these numbers:\n",
      "Task best_alpha: 0.765\n",
      "Task r2_train_simple_mix: 0.627255043446\n",
      "Task r2_test_simple_mix: 0.781144169579\n",
      "Task r2_train_stacking: 0.632175561459\n",
      "Task r2_test_stacking: 0.771297132342\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"\" # EMAIL HERE\n",
    "STUDENT_TOKEN = \"\" # TOKEN HERE\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
